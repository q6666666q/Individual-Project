services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    ports:
      - "80:9870"     
      - "9000:9000"   
    environment:
      CLUSTER_NAME: hadoop
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      CORE_CONF_io_compression_codecs: org.apache.hadoop.io.compress.SnappyCodec
      HDFS_CONF_dfs_replication: "2"                      
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: "false"
      HDFS_CONF_dfs_namenode_safemode_min_datanodes: "2"
      HDFS_CONF_dfs_namenode_safemode_extension: "10000"
    volumes:
      - infs3208_hadoop_namenode:/hadoop/dfs/name
    networks: [spark-net]
    restart: unless-stopped

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    hostname: datanode1
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      CORE_CONF_io_compression_codecs: org.apache.hadoop.io.compress.SnappyCodec
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
    volumes:
      - infs3208_hadoop_datanode1:/hadoop/dfs/data
    networks: [spark-net]
    depends_on: [namenode]
    restart: unless-stopped

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    hostname: datanode2
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      CORE_CONF_io_compression_codecs: org.apache.hadoop.io.compress.SnappyCodec
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
    volumes:
      - infs3208_hadoop_datanode2:/hadoop/dfs/data
    networks: [spark-net]
    depends_on: [namenode]
    restart: unless-stopped

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode3
    hostname: datanode3
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      CORE_CONF_io_compression_codecs: org.apache.hadoop.io.compress.SnappyCodec
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
    volumes:
      - infs3208_hadoop_datanode3:/hadoop/dfs/data
    networks: [spark-net]
    depends_on: [namenode]
    restart: unless-stopped

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    environment:
      - PYSPARK_PYTHON=python3    
    ports:
      - "8080:8080"   
      - "7077:7077"   
    networks: [spark-net]
    depends_on: [namenode, datanode1, datanode2, datanode3]
    restart: unless-stopped

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    ports:
      - "8081:8081"   
    networks: [spark-net]
    depends_on: [spark-master]
    restart: unless-stopped

  spark-worker-2:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-2
    hostname: spark-worker-2
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
    ports:
      - "8082:8081"   
    networks: [spark-net]
    depends_on: [spark-master]
    restart: unless-stopped

  jupyternb:
    image: jupyter/pyspark-notebook:python-3.10
    container_name: jupyternb
    hostname: jupyternb
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      - JUPYTER_TOKEN=               
      - PYSPARK_DRIVER_PYTHON=python3
      - PYSPARK_PYTHON=python3
    volumes:
      - ./work:/home/jovyan/work     
    ports:
      - "8888:8888"
    networks: [spark-net]
    depends_on: [spark-master]
    restart: unless-stopped

networks:
  spark-net:

volumes:
  infs3208_hadoop_namenode:
  infs3208_hadoop_datanode1:
  infs3208_hadoop_datanode2:
  infs3208_hadoop_datanode3:
